{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Natural image scene discrimination in mouse visual cortices\n",
    "applied linear svm classifier (L2 penalty) penalaty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import Libraries\n",
    "import libraries neded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from allensdk.core.brain_observatory_cache import BrainObservatoryCache\n",
    "import allensdk.brain_observatory.stimulus_info as stim_info\n",
    "import numpy as np\n",
    "import pprint\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import svm,cross_validation\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import shuffle\n",
    "from __future__ import print_function\n",
    "import h5py\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# This class uses a 'manifest' to keep track of downloaded data and metadata.  \n",
    "# All downloaded files will be stored relative to the directory holding the manifest\n",
    "# file.  If 'manifest_file' is a relative path (as it is below), it will be \n",
    "# saved relative to your working directory.  It can also be an absolute path.\n",
    "boc = BrainObservatoryCache(manifest_file='boc/manifest.json')\n",
    "area_labels = ['PM','AL', 'LM', 'V1']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Find Regularization parameter (C)\n",
    "Perform parameter search to find \"best\" regularization parameter C for the SVM. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Gridsearch CV\n",
    "\n",
    "areas = [ 'VISpm', 'VISal', 'VISl', 'VISp']\n",
    "nfolds = 3\n",
    "C_params = list()\n",
    "best_scores = list()\n",
    "test_scores = list()\n",
    "stim_onset = 7\n",
    "\n",
    "C_s = np.concatenate((np.logspace(-7, -0.05, 5),[1],np.logspace(0.05, 7, 5)))\n",
    "\n",
    "for aa in range(len(areas)):\n",
    "    start_total = datetime.datetime.now()\n",
    "    \n",
    "    this_area = areas[aa]\n",
    "    filename = this_area + '_sweeps_data.h5'\n",
    "\n",
    "    with h5py.File(filename,'r') as hf:\n",
    "        data_1 = hf.get('X_matrix_time')\n",
    "        XT_ALL = np.array(data_1)\n",
    "        data_2 = hf.get('Y_matrix')\n",
    "        Y_ALL = np.array(data_2)\n",
    "        \n",
    "    y = np.median(Y_ALL,axis=0).astype(int)[50:] #first 50 trials belongs to blank, ignore     \n",
    "    X_range = XT_ALL[50:,:,stim_onset+6:stim_onset+10] #select range approximately 200 to 300ms from stimulus onset\n",
    "    X_ALL = np.mean(X_range,axis=2) #compute euclidean norm\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X_ALL,y,test_size=0.3)   \n",
    "    \n",
    "    #scale each feature [0,1]\n",
    "    if scale:\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "        X_train_scale = scaler.fit_transform(X_train)\n",
    "        X_test_scale = scaler.fit_transform(X_test)\n",
    "    else:\n",
    "        #normalize the response of each neuron to zero mean and unit variance\n",
    "        #this uses a lot of memory. consider running on faster machine\n",
    "        scaler = preprocessing.StandardScaler(copy=False).transform(X_train)\n",
    "        X_train_scale = scaler.transform(X_train)\n",
    "        X_test_scale = scaler.transform(X_test)\n",
    "            \n",
    "    SVC = LinearSVC()\n",
    "    clf = GridSearchCV(estimator=SVC, param_grid=dict(C=C_s), n_jobs=1,cv=nfolds,scoring='accuracy')\n",
    "    clf.fit(X_train, y_train)        \n",
    "    \n",
    "    end_total = datetime.datetime.now()\n",
    "    print('elapsed time: %d seconds' %(end_total - start_total).total_seconds())\n",
    "    \n",
    "    C_params.append(clf.best_estimator_.C)\n",
    "    best_scores.append(100 * clf.best_score_)\n",
    "    test_scores.append(100 * clf.score(X_test,y_test))\n",
    "    \n",
    "    print(this_area + ' best score: %0.2f' %(100 * clf.best_score_))\n",
    "    print(this_area + ' best estimator: %0.4f' %(clf.best_estimator_.C))\n",
    "    print(this_area + ' test score: %0.2f' %(100 * clf.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classification accuracy per visual area and across time\n",
    "trained classifier with mean activity from 200-300ms post-stimulus onset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the previous version of the code below, included training data when testing the decoder performance across time. Now the train_test_split is done on the 3D matrix (XTn) on each iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VISpm, mean test accuracy: 24.31 (+/- 0.65), mean shuffle accuracy: 0.94 (+/- 0.09)\n",
      "VISal, mean test accuracy: 25.95 (+/- 1.13), mean shuffle accuracy: 0.68 (+/- 0.08)"
     ]
    }
   ],
   "source": [
    "colors = ['red','blue','green','cyan']\n",
    "areas = [ 'VISpm', 'VISal', 'VISl', 'VISp']\n",
    "\n",
    "#first 7 frames before stimulus, 7 stimulus ON frames followed by 7 frames post stimulus.\n",
    "#please note that in the actual experiment there was no \n",
    "#blank period between frames, i.e. stimuli were presented consecutively. \n",
    "stim_onset = 7 #occurs after first 7 frames\n",
    "stim_frames = 7\n",
    "\n",
    "niter = 5     #run classifier. repeat 10x and average\n",
    "num_cells_per_area = list()\n",
    "acq_fps = float(30.1)\n",
    "limit_num_neurons = 0\n",
    "time_test_subset = 1\n",
    "single_frame = 1\n",
    "scale = 1\n",
    "add_filename = '_mean'\n",
    "\n",
    "if limit_num_neurons:\n",
    "    num_neurons_include = 2100 \n",
    "    add_filename = add_filename + '_2100_neurons'\n",
    "\n",
    "for aa in range(len(areas)):\n",
    "    this_area = areas[aa]\n",
    "    filename = this_area + '_sweeps_data.h5'\n",
    "\n",
    "    #load hdf5 file\n",
    "    with h5py.File(filename,'r') as hf:\n",
    "        data_1 = hf.get('X_matrix_time')\n",
    "        X = np.array(data_1)\n",
    "        data_2 = hf.get('Y_matrix')\n",
    "        Y_ALL = np.array(data_2)\n",
    "    \n",
    "    XT_ALL = X[50:,:,:] #first 50 trials belongs to blank, ignore \n",
    "    y = np.median(Y_ALL,axis=0).astype(int)[50:] #first 50 trials belongs to blank, ignore \n",
    "    y_shuffle = shuffle(y)\n",
    "    \n",
    "    num_cells_per_area.append(X.shape[1])\n",
    "    accuracy_per_iter = list()\n",
    "    accuracy_per_iter_shuffle = list()\n",
    "    \n",
    "    num_neurons_include = num_cells_per_area[aa]\n",
    "\n",
    "    for ii in range(niter):\n",
    "        inds = np.random.randint(0,num_neurons_include,size=num_neurons_include) #shuffle or randomly pick neurons to include\n",
    "        XTn = XT_ALL[:,inds,:]\n",
    "\n",
    "        #for cross validation do, 70/30 split. split 3D matrix\n",
    "        XTn_train,XTn_test,y_train,y_test = train_test_split(XTn,y,test_size=0.3) \n",
    "        \n",
    "        #select range approximately 200 to 300ms from stimulus onset, and compute mean\n",
    "        X_train = np.mean(XTn_train[:,:,stim_onset+6:stim_onset+10],axis=2)\n",
    "        X_test = np.mean(XTn_test[:,:,stim_onset+6:stim_onset+10],axis=2)\n",
    "\n",
    "        if single_frame: #pick single frame\n",
    "            #stim_onset+6 = approx 200ms post stimulus onset, stim_onset+9 ~approx 300ms post stim\n",
    "            X_train = XTn_train[:,:,stim_onset+6]\n",
    "            X_test = XTn_test[:,:,stim_onset+6]\n",
    "            add_filename = '_single_frame'\n",
    "        \n",
    "        y_train_shuffle = shuffle(y_train)\n",
    "        y_test_shuffle = shuffle(y_test)\n",
    "        \n",
    "        #scale each feature [0,1]\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "        X_train_scale = scaler.fit_transform(X_train)\n",
    "        X_test_scale = scaler.fit_transform(X_test)\n",
    "       \n",
    "        if not scale:\n",
    "            #normalize the response of each neuron to zero mean and unit variance\n",
    "            #this uses a lot of memory. consider running on faster machine\n",
    "            scaler = preprocessing.StandardScaler(copy=False).transform(X_train)\n",
    "            X_train_scale = scaler.transform(X_train)\n",
    "            X_test_scale = scaler.transform(X_test)\n",
    "            \n",
    "        # train linear SVM classifier on data - mean response during 200-300ms post stimulus\n",
    "        linearSVM_1 = OneVsRestClassifier(LinearSVC(C=0.0163))\n",
    "        linearSVM_1.fit(X_train_scale, y_train)\n",
    "        accuracy_per_iter.append(100*linearSVM_1.score(X_test_scale,y_test))\n",
    "        \n",
    "        # train linear SVM classifier on shuffled responses\n",
    "        linearSVM_1_shuffle = OneVsRestClassifier(LinearSVC(C=0.0163))\n",
    "        linearSVM_1_shuffle.fit(X_train_scale, y_train_shuffle)\n",
    "        accuracy_per_iter_shuffle.append(100*linearSVM_1_shuffle.score(X_test_scale,y_test_shuffle)) \n",
    "\n",
    "        # test decoding accuracy across time on test data\n",
    "        window_frames_to_plot = XTn_test.shape[2]\n",
    "        accuracy_time = list()\n",
    "        accuracy_time_shuffle = list()\n",
    "        \n",
    "        for w in np.arange(0,window_frames_to_plot):\n",
    "            X_test_time = XTn_test[:,:,w]\n",
    "            X_test_scale = scaler.transform(X_test_time)\n",
    "            accuracy_time.append(100*linearSVM_1.score(X_test_scale,y_test))\n",
    "            accuracy_time_shuffle.append(100*linearSVM_1_shuffle.score(X_test_scale,y_test_shuffle))\n",
    "\n",
    "        if ii == 0:\n",
    "            accuracy_time_all = accuracy_time\n",
    "            accuracy_time_all_shuffle = accuracy_time_shuffle\n",
    "        if ii > 0:\n",
    "            accuracy_time_all = np.vstack((accuracy_time_all,np.array(accuracy_time)))\n",
    "            accuracy_time_all_shuffle = np.vstack((accuracy_time_all_shuffle,np.array(accuracy_time_shuffle)))\n",
    "\n",
    "    mean_accuracy = np.mean(np.array(accuracy_per_iter))\n",
    "    std_accuracy = np.std(np.array(accuracy_per_iter)) / np.sqrt(niter)    \n",
    "    mean_accuracy_shuffle = np.mean(np.array(accuracy_per_iter_shuffle))\n",
    "    std_accuracy_shuffle = np.std(np.array(accuracy_per_iter_shuffle)) / np.sqrt(niter)    \n",
    "\n",
    "    print(this_area + ', mean test accuracy: %0.2f (+/- %0.2f), mean shuffle accuracy: %0.2f (+/- %0.2f)'\n",
    "          %(mean_accuracy,std_accuracy,mean_accuracy_shuffle,std_accuracy_shuffle))\n",
    "\n",
    "    mean_accuracy_time = np.mean(accuracy_time_all,axis=0)\n",
    "    std_accuracy_time = np.std(accuracy_time_all,axis=0) / np.sqrt(niter)\n",
    "    mean_accuracy_time_shuffle = np.mean(accuracy_time_all_shuffle,axis=0)\n",
    "    std_accuracy_time_shuffle = np.std(accuracy_time_all_shuffle,axis=0) / np.sqrt(niter)\n",
    "    \n",
    "    if aa == 0:\n",
    "        mean_accuracy_time_all = mean_accuracy_time\n",
    "        std_accuracy_time_all = std_accuracy_time\n",
    "        mean_accuracy_per_area = mean_accuracy\n",
    "        std_accuracy_per_area = std_accuracy\n",
    "        \n",
    "        mean_accuracy_time_all_shuffle = mean_accuracy_time_shuffle\n",
    "        std_accuracy_time_all_shuffle = std_accuracy_time_shuffle\n",
    "        mean_accuracy_per_area_shuffle = mean_accuracy_shuffle\n",
    "        std_accuracy_per_area_shuffle = std_accuracy_shuffle\n",
    "        \n",
    "    if aa > 0:\n",
    "        mean_accuracy_time_all = np.vstack((mean_accuracy_time_all,mean_accuracy_time))\n",
    "        std_accuracy_time_all = np.vstack((std_accuracy_time_all,std_accuracy_time))\n",
    "        mean_accuracy_per_area = np.vstack((mean_accuracy_per_area,mean_accuracy))\n",
    "        std_accuracy_per_area = np.vstack((std_accuracy_per_area,std_accuracy))\n",
    "        \n",
    "        mean_accuracy_time_all_shuffle = np.vstack((mean_accuracy_time_all_shuffle,mean_accuracy_time_shuffle))\n",
    "        std_accuracy_time_all_shuffle = np.vstack((std_accuracy_time_all_shuffle,std_accuracy_time_shuffle))\n",
    "        mean_accuracy_per_area_shuffle = np.vstack((mean_accuracy_per_area_shuffle,mean_accuracy_shuffle))\n",
    "        std_accuracy_per_area_shuffle = np.vstack((std_accuracy_per_area_shuffle,std_accuracy_shuffle))\n",
    "        \n",
    "    num_classes = np.max(linearSVM_1.classes_)\n",
    "\n",
    "if time_test_subset:\n",
    "    add_filename = add_filename + '_test_subset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#make pretty plots\n",
    "num_areas =len(areas)\n",
    "inds =np.arange(num_areas) \n",
    "chance_val = 100 /float(1 + num_classes)\n",
    "xtime = (1/acq_fps)*(np.arange(-7,14))\n",
    "\n",
    "fig = plt.figure(figsize=(18,8))\n",
    "ax = plt.subplot(121)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.get_xaxis().tick_bottom()\n",
    "ax.get_yaxis().tick_left()\n",
    "ax.tick_params(axis='both',direction='out')\n",
    "\n",
    "plt.ylabel('Accuracy (%)',fontsize=16)\n",
    "plt.xlabel('areas ', fontsize=16)\n",
    "bar_width = 0.25\n",
    "\n",
    "bar1 = plt.bar(inds,mean_accuracy_per_area,alpha=0.5, yerr=std_accuracy_per_area[:,0],\n",
    "               width=bar_width)\n",
    "bar2 = plt.bar(inds+bar_width,mean_accuracy_per_area_shuffle,alpha=0.5, \n",
    "               yerr=std_accuracy_per_area_shuffle[:,0],\n",
    "               width=bar_width, color='black')\n",
    "chance = chance_val*np.ones(len(areas))\n",
    "xs = np.linspace(0,num_areas,num=num_areas)\n",
    "plt.plot(xs,chance,ls='--',color='black',label='chance')\n",
    "plt.xticks(inds + 0.5 * bar_width,area_labels)\n",
    "plt.grid()\n",
    "ax.set_ylim(0,100)\n",
    "\n",
    "def autolabel(rects):\n",
    "    # attach some text labels\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,'%0.02f' \n",
    "                %(height),ha='center', va='bottom')\n",
    "autolabel(bar1)\n",
    "autolabel(bar2)\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.get_xaxis().tick_bottom()\n",
    "ax.get_yaxis().tick_left()\n",
    "ax.tick_params(axis='both',direction='out')\n",
    "ax.set_ylim(0,100)\n",
    "\n",
    "for aa in range(len(areas)):\n",
    "    plt.plot(xtime,mean_accuracy_time_all[aa,:],label=area_labels[aa],color=colors[aa])\n",
    "    upper = mean_accuracy_time_all[aa,:] + std_accuracy_time_all[aa,:]\n",
    "    lower = mean_accuracy_time_all[aa,:] - std_accuracy_time_all[aa,:]\n",
    "    plt.fill_between(xtime,lower,upper,alpha=0.25,color=colors[aa])\n",
    "    \n",
    "    plt.plot(xtime,mean_accuracy_time_all_shuffle[aa,:],label=area_labels[aa]+'-shuffle',\n",
    "             ls='--',color=colors[aa])\n",
    "    upper_shuffle = mean_accuracy_time_all_shuffle[aa,:] + std_accuracy_time_all[aa,:]\n",
    "    lower_shuffle = mean_accuracy_time_all_shuffle[aa,:] - std_accuracy_time_all[aa,:]\n",
    "    plt.fill_between(xtime,lower_shuffle,upper_shuffle,alpha=0.25,color=colors[aa])\n",
    "\n",
    "plt.ylabel('Accuracy (%)',fontsize=16)\n",
    "plt.xlabel('time from stimulus onset (ms)', fontsize=16)\n",
    "\n",
    "chance = chance_val*np.ones(window_frames_to_plot)\n",
    "plt.plot(xtime,chance,ls='--',color='black',label='chance')\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc=2, borderaxespad=0.,frameon=False,fontsize=16)\n",
    "\n",
    "with PdfPages('Decoder accuracy all visual areas'+ add_filename + '.pdf') as pdf:\n",
    "    pdf.savefig(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. decoder accuracy per number of neurons - per area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colors = ['red','blue','green','cyan']\n",
    "areas = [ 'VISpm', 'VISal', 'VISl', 'VISp']\n",
    "niter = 10\n",
    "scale = 1\n",
    "\n",
    "for aa in range(len(areas)):\n",
    "    this_area = areas[aa]\n",
    "    filename = this_area + '_sweeps_data.h5'\n",
    "\n",
    "    with h5py.File(filename,'r') as hf:\n",
    "        data_1 = hf.get('X_matrix_time')\n",
    "        XT_ALL = np.array(data_1)\n",
    "        data_2 = hf.get('Y_matrix')\n",
    "        Y_ALL = np.array(data_2)\n",
    "\n",
    "    y = np.median(Y_ALL,axis=0).astype(int)[50:] #select ignore first 50 trials which belong to blank\n",
    "    y_shuffle = shuffle(y)\n",
    "    #select range approximately 200 to 300ms from stimulus onset, ignore first 50 trials\n",
    "    X_range = XT_ALL[50:,:,stim_onset+6:stim_onset+10] \n",
    "    #compute mean\n",
    "    X_ALL = np.mean(X_range,axis=2)\n",
    "    add_filename = '_mean_'\n",
    "\n",
    "    total_num_cells = X_ALL.shape[1]\n",
    "    percentages = np.array([0.03,1,5, 10, 25, 50, 75, 100])\n",
    "    num_neurons_list =list(np.rint(total_num_cells * percentages/float(100)))\n",
    "\n",
    "    accuracy_per_neuron_count = np.empty([niter,len(num_neurons_list)])\n",
    "    accuracy_per_neuron_count[:] = np.nan\n",
    "    accuracy_per_neuron_count_shuffle = np.empty([niter,len(num_neurons_list)])\n",
    "    accuracy_per_neuron_count_shuffle[:] = np.nan\n",
    "\n",
    "    for ii in range(niter):\n",
    "        for nn in range(len(num_neurons_list)):\n",
    "            this_num_neurons = int(num_neurons_list[nn])\n",
    "            inds = np.random.randint(0,total_num_cells,(this_num_neurons,))\n",
    "            Xn = X_ALL[:,inds]\n",
    "            #randomly select 30% of samples for test dataset\n",
    "            Xn_train,Xn_test,yn_train,yn_test = train_test_split(Xn,y,test_size=0.3)\n",
    "            \n",
    "            if scale:\n",
    "                scaler = preprocessing.MinMaxScaler()\n",
    "                Xn_train_scale = scaler.fit_transform(Xn_train)\n",
    "                Xn_test_scale = scaler.fit_transform(Xn_test)\n",
    "            else:\n",
    "                #normalize data to zero mean and unit variance\n",
    "                scaler = preprocessing.StandardScaler(copy=False).fit(Xn_train)\n",
    "                Xn_train_scale = scaler.transform(Xn_train)\n",
    "                Xn_test_scale = scaler.transform(Xn_test)\n",
    "\n",
    "            # train linear SVM classifier - to data\n",
    "            linearSVM = OneVsRestClassifier(LinearSVC(C=0.0163))\n",
    "            linearSVM.fit(Xn_train_scale, yn_train)\n",
    "            accuracy = 100*linearSVM.score(Xn_test_scale,yn_test) \n",
    "            accuracy_per_neuron_count[ii,nn] = accuracy\n",
    "            \n",
    "            #train linear SVM on shuffled data\n",
    "            yn_train_shuffle = shuffle(yn_train)\n",
    "            yn_test_shuffle = shuffle(yn_test)\n",
    "            linearSVM_shuffle = OneVsRestClassifier(LinearSVC(C=0.0163))\n",
    "            linearSVM_shuffle.fit(Xn_train_scale, yn_train_shuffle)\n",
    "            accuracy_shuffle = 100*linearSVM_shuffle.score(Xn_test_scale,yn_test_shuffle) \n",
    "            accuracy_per_neuron_count_shuffle[ii,nn] = accuracy_shuffle\n",
    "\n",
    "    #compute mean and std of accuracy over iterations for data and shuffle\n",
    "    mean_accuracy = np.nanmean(accuracy_per_neuron_count,axis=0)\n",
    "    std_accuracy = np.nanstd(accuracy_per_neuron_count,axis=0) / np.sqrt(niter)\n",
    "    mean_accuracy_shuffle = np.nanmean(accuracy_per_neuron_count_shuffle,axis=0)\n",
    "    std_accuracy_shuffle = np.nanstd(accuracy_per_neuron_count_shuffle,axis=0) / np.sqrt(niter)\n",
    "    \n",
    "    if aa == 0:\n",
    "        mean_accuracy_per_neuron_count_per_area = mean_accuracy\n",
    "        std_accuracy_per_neuron_count_per_area = std_accuracy\n",
    "        mean_accuracy_per_neuron_count_per_area_shuffle = mean_accuracy_shuffle\n",
    "        std_accuracy_per_neuron_count_per_area_shuffle = std_accuracy_shuffle\n",
    "        neuron_count_per_area = num_neurons_list\n",
    "\n",
    "    else:\n",
    "        mean_accuracy_per_neuron_count_per_area = np.vstack((mean_accuracy_per_neuron_count_per_area,mean_accuracy))\n",
    "        std_accuracy_per_neuron_count_per_area = np.vstack((std_accuracy_per_neuron_count_per_area,std_accuracy))\n",
    "        mean_accuracy_per_neuron_count_per_area_shuffle = np.vstack((mean_accuracy_per_neuron_count_per_area_shuffle, \n",
    "                                                                     mean_accuracy_shuffle))\n",
    "        std_accuracy_per_neuron_count_per_area_shuffle = np.vstack((std_accuracy_per_neuron_count_per_area_shuffle, \n",
    "                                                                    std_accuracy_shuffle))\n",
    "        neuron_count_per_area = np.vstack((neuron_count_per_area,num_neurons_list))    \n",
    "    print(this_area)\n",
    "    \n",
    "print('-----Done-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make pretty plots\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = plt.subplot(111)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.get_xaxis().tick_bottom()\n",
    "ax.get_yaxis().tick_left()\n",
    "ax.tick_params(axis='both',direction='out')\n",
    "\n",
    "ax.set_ylim(0,100)\n",
    "plt.ylabel('Classification Accuracy (%)',fontsize=16)\n",
    "plt.xlabel('(%) Neurons in Area ', fontsize=16)\n",
    "plt.title( 'Accuracy vs # of Neurons',fontsize=16)\n",
    "\n",
    "for aa in range(len(areas)):\n",
    "    plt.plot(percentages,mean_accuracy_per_neuron_count_per_area[aa,:],ls='-',\n",
    "             label=area_labels[aa],color=colors[aa])\n",
    "    upper = mean_accuracy_per_neuron_count_per_area[aa,:] + std_accuracy_per_neuron_count_per_area[aa,:]\n",
    "    lower = mean_accuracy_per_neuron_count_per_area[aa,:] - std_accuracy_per_neuron_count_per_area[aa,:]\n",
    "    plt.fill_between(percentages,lower,upper,alpha=0.15,color=colors[aa])\n",
    "    \n",
    "    plt.plot(percentages,mean_accuracy_per_neuron_count_per_area_shuffle[aa,:],ls='--',\n",
    "             label=area_labels[aa] + '-shuffle',color = colors[aa])\n",
    "    upper_shuffle = mean_accuracy_per_neuron_count_per_area_shuffle[aa,:] + std_accuracy_per_neuron_count_per_area_shuffle[aa,:]\n",
    "    lower_shuffle = mean_accuracy_per_neuron_count_per_area_shuffle[aa,:] - std_accuracy_per_neuron_count_per_area_shuffle[aa,:]\n",
    "    plt.fill_between(percentages,lower_shuffle,upper_shuffle,alpha=0.15,color=colors[aa])\n",
    "\n",
    "chance = (100/float(119))*np.ones(len(percentages))\n",
    "plt.plot(percentages,chance,ls='--',color='black',label='chance')\n",
    "plt.legend(bbox_to_anchor=(0.995, 1), loc=2, borderaxespad=0.,frameon=False,fontsize=14)\n",
    "plt.yticks([0, 25 ,50,75, 100])\n",
    "plt.grid()\n",
    "\n",
    "with PdfPages('Decoder accuracy per number neurons per visual area' + add_filename + '.pdf') as pdf:\n",
    "    pdf.savefig(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. train & test classifier at each time point - per area\n",
    "consider running on cluster/gpu. this takes awhile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colors = ['red','blue','green','cyan']\n",
    "areas = [ 'VISpm', 'VISal', 'VISl', 'VISp']\n",
    "#first 28 frames is considered baseline for calculating df/f,followed by 7 stimulus ON frames\n",
    "# followed by 28 frames post stimulus. please note that in the actual experiment there was no \n",
    "#blank period between frames, i.e. stimuli were presented consecutively. \n",
    "stim_onset = 7 #occurs after first 7 frames\n",
    "stim_frames = 7\n",
    "\n",
    "niter = 3     #run classifier with different cross validation\n",
    "num_cells_per_area = np.empty(len(areas))\n",
    "num_cells_per_area[:] = np.nan\n",
    "acq_fps = float(30.1)\n",
    "scale = 1\n",
    "add_to_filename = '_all_'\n",
    "num_cells_per_area  = list()\n",
    "\n",
    "for aa in range(len(areas)):\n",
    "    this_area = areas[aa]\n",
    "    filename = this_area + '_sweeps_data.h5'\n",
    "\n",
    "    #load hdf5 file\n",
    "    with h5py.File(filename,'r') as hf:\n",
    "        data_1 = hf.get('X_matrix_time')\n",
    "        XT_ALL = np.array(data_1)\n",
    "        data_2 = hf.get('Y_matrix')\n",
    "        Y_ALL = np.array(data_2)\n",
    "        data_3 = hf.get('cell_IDs')\n",
    "        cell_IDs = np.array(data_3)\n",
    "\n",
    "    y = np.median(Y_ALL,axis=0).astype(int)[50:] #select ignore first 50 trials which belong to blank\n",
    "    y_shuffle = shuffle(y)\n",
    "    #select range for testing classifier across time: 250 ms (7 frames) \n",
    "    #before stim onset & 250ms post stimulus onset (14 frames post stimulus onset)\n",
    "    XT_test_range = XT_ALL[50:,:,:]\n",
    "    frames_to_test = XT_test_range.shape[2]\n",
    "    num_cells_per_area.append(len(cell_IDs))\n",
    "    num_neurons_include = num_cells_per_area[aa]\n",
    "           \n",
    "    for ii in range(niter):\n",
    "        inds = np.random.randint(0,num_neurons_include,size=num_neurons_include)\n",
    "        XTn = XT_test_range[:,inds,:]\n",
    "        accuracy_time = list()\n",
    "        accuracy_time_shuffle = list()\n",
    "        \n",
    "        for w in np.arange(0,frames_to_test):\n",
    "            X_frame = XTn[:,:,w]\n",
    "            \n",
    "            #for cross validation do, 70/30 split.\n",
    "            X_train,X_test,y_train,y_test = train_test_split(X_frame,y,test_size=0.3) \n",
    "            y_train_shuffle = shuffle(y_train)\n",
    "            y_test_shuffle = shuffle(y_test)\n",
    "            \n",
    "            if scale: #scale from [0,1]\n",
    "                scaler = preprocessing.MinMaxScaler()\n",
    "                X_train_scale = scaler.fit_transform(X_train)\n",
    "                X_test_scale = scaler.fit_transform(X_test)\n",
    "            else:\n",
    "                #normalize the response of each neuron to zero mean and unit variance\n",
    "                scaler = preprocessing.StandardScaler(copy=False).fit(X_train)\n",
    "                X_train_scale = scaler.transform(X_train)\n",
    "                X_test_scale = scaler.transform(X_test)\n",
    "            \n",
    "            # train linear SVM classifier on data\n",
    "            linearSVM_1 = OneVsRestClassifier(LinearSVC(C=0.0163))\n",
    "            linearSVM_1.fit(X_train_scale, y_train)\n",
    "            # train linear SVM classifier on shuffled data\n",
    "            linearSVM_1_shuffle = OneVsRestClassifier(LinearSVC(C=0.0163))\n",
    "            linearSVM_1_shuffle.fit(X_train_scale, y_train_shuffle)\n",
    "            \n",
    "            accuracy_time.append((100*linearSVM_1.score(X_test_scale,y_test)))\n",
    "            accuracy_time_shuffle.append((100*linearSVM_1_shuffle.score(X_test_scale,y_test_shuffle))) \n",
    "\n",
    "        if ii == 0:\n",
    "            accuracy_time_all = np.array(accuracy_time)\n",
    "            accuracy_time_all_shuffle = np.array(accuracy_time_shuffle)\n",
    "        if ii > 0:\n",
    "            accuracy_time_all = np.vstack((accuracy_time_all,np.array(accuracy_time)))\n",
    "            accuracy_time_all_shuffle = np.vstack((accuracy_time_all_shuffle,np.array(accuracy_time_shuffle)))\n",
    "\n",
    "    mean_accuracy_time = np.mean(accuracy_time_all,axis=0)\n",
    "    std_accuracy_time = np.std(accuracy_time_all,axis=0) / np.sqrt(niter)\n",
    "    mean_accuracy_time_shuffle = np.mean(accuracy_time_all_shuffle,axis=0)\n",
    "    std_accuracy_time_shuffle = np.std(accuracy_time_all_shuffle,axis=0) / np.sqrt(niter)\n",
    "    \n",
    "    if aa == 0:\n",
    "        mean_accuracy_time_all = mean_accuracy_time\n",
    "        std_accuracy_time_all = std_accuracy_time\n",
    "        \n",
    "        mean_accuracy_time_all_shuffle = mean_accuracy_time_shuffle\n",
    "        std_accuracy_time_all_shuffle = std_accuracy_time_shuffle\n",
    "        \n",
    "    if aa > 0:\n",
    "        mean_accuracy_time_all = np.vstack((mean_accuracy_time_all,mean_accuracy_time))\n",
    "        std_accuracy_time_all = np.vstack((std_accuracy_time_all,std_accuracy_time))\n",
    "        mean_accuracy_time_all_shuffle = np.vstack((mean_accuracy_time_all_shuffle,mean_accuracy_time_shuffle))\n",
    "        std_accuracy_time_all_shuffle = np.vstack((std_accuracy_time_all_shuffle,std_accuracy_time_shuffle))\n",
    "      \n",
    "    num_classes = np.max(linearSVM_1.classes_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make pretty plots\n",
    "num_areas =len(areas)\n",
    "inds =np.arange(num_areas)\n",
    "chance_val = 100 /float(1 + num_classes)\n",
    "xtime = (1/acq_fps)*(np.arange(-7,14))\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = plt.subplot(111)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.get_xaxis().tick_bottom()\n",
    "ax.get_yaxis().tick_left()\n",
    "ax.tick_params(axis='both',direction='out')\n",
    "\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.get_xaxis().tick_bottom()\n",
    "ax.get_yaxis().tick_left()\n",
    "ax.tick_params(axis='both',direction='out')\n",
    "ax.set_ylim(0,100)\n",
    "\n",
    "trange = np.arange(0,len(xtime))\n",
    "for aa in range(len(areas)):\n",
    "    plt.plot(xtime[trange],mean_accuracy_time_all[aa,trange],label=area_labels[aa],color=colors[aa])\n",
    "    upper = mean_accuracy_time_all[aa,trange] + std_accuracy_time_all[aa,trange]\n",
    "    lower = mean_accuracy_time_all[aa,trange] - std_accuracy_time_all[aa,trange]\n",
    "    plt.fill_between(xtime[trange],lower,upper,alpha=0.25,color=colors[aa])\n",
    "    \n",
    "    plt.plot(xtime[0:15],mean_accuracy_time_all_shuffle[aa,0:15],label=area_labels[aa]+'-shuffle',\n",
    "             ls='--',color=colors[aa])\n",
    "    upper_shuffle = mean_accuracy_time_all_shuffle[aa,trange] + std_accuracy_time_all[aa,trange]\n",
    "    lower_shuffle = mean_accuracy_time_all_shuffle[aa,trange] - std_accuracy_time_all[aa,trange]\n",
    "    plt.fill_between(xtime[trange],lower_shuffle,upper_shuffle,alpha=0.25,color=colors[aa])\n",
    "\n",
    "plt.ylabel('Accuracy (%)',fontsize=16)\n",
    "plt.xlabel('time from stimulus onset (ms)', fontsize=16)\n",
    "\n",
    "chance = chance_val*np.ones(frames_to_test)\n",
    "plt.plot(xtime,chance,ls='--',color='black',label='chance')\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc=2, borderaxespad=0.,frameon=False,fontsize=16)\n",
    "\n",
    "with PdfPages('Decoder accuracy across time'+ add_to_filename +'.pdf') as pdf:\n",
    "    pdf.savefig(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Cell type-specific classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "areas = [ 'VISpm', 'VISal', 'VISl', 'VISp']\n",
    "cre_lines = ['Cux2-CreERT2','Rorb-IRES2-Cre','Rbp4-Cre'] #layer 2/3 & 4, layer 4, layer 5\n",
    "add_filename = '_all_neurons'\n",
    "\n",
    "niter = 5\n",
    "acq_fps = float(30.1)\n",
    "num_areas = len(areas)\n",
    "num_cre_lines = len(cre_lines)\n",
    "scale = 1\n",
    "limit_neurons = 0\n",
    "#first 7 frames before stimulus, 7 stimulus ON frames followed by 7 frames post stimulus.\n",
    "#please note that in the actual experiment there was no \n",
    "#blank period between frames, i.e. stimuli were presented consecutively. \n",
    "stim_onset = 7 #occurs after first 7 frames\n",
    "stim_frames = 7\n",
    "\n",
    "#intialize variables for storage\n",
    "mean_accuracy_per_cre_area = np.empty([num_areas,num_cre_lines])\n",
    "mean_accuracy_per_cre_area[:] = np.nan\n",
    "std_accuracy_per_cre_area = np.empty([num_areas,num_cre_lines])\n",
    "std_accuracy_per_cre_area[:] = np.nan\n",
    "num_cells_per_cre_area = np.empty([num_areas,num_cre_lines])\n",
    "num_cells_per_cre_area[:] = np.nan\n",
    "\n",
    "mean_accuracy_time_per_cre_area = np.empty([num_cre_lines,21,num_areas]) #21 frames included\n",
    "mean_accuracy_time_per_cre_area[:] = np.nan\n",
    "std_accuracy_time_per_cre_area = np.empty([num_cre_lines,21,num_areas]) #21 frames included\n",
    "std_accuracy_time_per_cre_area[:] = np.nan\n",
    "\n",
    "for aa in range(num_areas):\n",
    "    this_area = areas[aa]\n",
    "    \n",
    "    for c in range(num_cre_lines):\n",
    "        this_cre = cre_lines[c]\n",
    "        this_filename = this_area + '_'+ this_cre +'_sweeps_data.h5'\n",
    "        with h5py.File(this_filename,'r') as hf:\n",
    "            data_1 = hf.get('X_matrix_time')\n",
    "            XT_ALL = np.array(data_1)\n",
    "            data_2 = hf.get('Y_matrix')\n",
    "            Y_ALL = np.array(data_2)\n",
    "    \n",
    "        y = np.median(Y_ALL,axis=0).astype(int)[50:] #select ignore first 50 trials which belong to blank\n",
    "        XT_ALL = XT_ALL[50:,:,:]\n",
    "        \n",
    "        num_cells_per_cre_area[aa,c] = XT_ALL.shape[1]\n",
    "        num_neurons_include = num_cells_per_cre_area[aa,c]\n",
    "\n",
    "        if limit_neurons:\n",
    "            print('-----' + str(200) + ' neurons limit-----')\n",
    "            num_neurons_include = 200\n",
    "            add_filename = '_200_neurons'\n",
    "            \n",
    "        accuracy_per_iter = list()\n",
    "        \n",
    "        #run classifier \n",
    "        for ii in range(niter):\n",
    "            inds = np.random.randint(0,num_neurons_include,size=num_neurons_include)\n",
    "            XT = XT_ALL[:,inds,:]\n",
    "        \n",
    "            #for cross validation do, 70/30 split. repeat 10 times and average\n",
    "            XT_train,XT_test,y_train,y_test = train_test_split(XT,y,test_size=0.3)\n",
    "            #select range approximately 200 to 300ms from stimulus onset, compute mean\n",
    "            X_train = np.mean(XT_train[:,:,stim_onset+6:stim_onset+10],axis=2) \n",
    "            X_test = np.mean(XT_test[:,:,stim_onset+6:stim_onset+10],axis=2)\n",
    "            \n",
    "            #scale data X [0,1] - important to scale data. prevents features with high values from dominating. \n",
    "            scaler = preprocessing.MinMaxScaler()\n",
    "            X_train_scale = scaler.fit_transform(X_train)\n",
    "            X_test_scale = scaler.transform(X_test)\n",
    "            if not scale:\n",
    "                #normalize the response of each neuron to zero mean and unit variance\n",
    "                scaler = preprocessing.StandardScaler(copy=False).fit(X_train)\n",
    "                X_train_scale = scaler.transform(X_train)\n",
    "                X_test_scale = scaler.transform(X_test)\n",
    "                \n",
    "            # linear SVM classifier - to data\n",
    "            linearSVM_2 = OneVsRestClassifier(LinearSVC(C=0.0163))\n",
    "            linearSVM_2.fit(X_train_scale, y_train)\n",
    "            accuracy_per_iter.append(100*linearSVM_2.score(X_test_scale,y_test)) \n",
    "\n",
    "            # decoding accuracy across time\n",
    "            time_points = XT_train.shape[2]\n",
    "            accuracy_time = list()\n",
    "            accuracy_time_shuffle = []\n",
    "            \n",
    "            for w in range(time_points):\n",
    "                X_test_time = XT_test[:,:,w]\n",
    "                X_test_scale = scaler.transform(X_test_time)\n",
    "                accuracy_time.append(100*linearSVM_2.score(X_test_scale,y_test))\n",
    "            \n",
    "            if ii == 0:\n",
    "                accuracy_time_all_iter = np.array(accuracy_time)\n",
    "            if ii > 0:\n",
    "                accuracy_time_all_iter = np.vstack((accuracy_time_all_iter,np.array(accuracy_time)))\n",
    "        \n",
    "        #mean accuracy over iterations\n",
    "        mean_accuracy_per_cre_area[aa,c] = np.mean(np.array(accuracy_per_iter))\n",
    "        std_accuracy_per_cre_area[aa,c] = np.std(np.array(accuracy_per_iter)) / (np.sqrt(niter)) \n",
    "        \n",
    "        #mean accuracy across time over iterations\n",
    "        mean_accuracy_time_per_cre_area[c,:,aa] = np.mean(accuracy_time_all_iter, axis=0)\n",
    "        std_accuracy_time_per_cre_area[c,:,aa] = np.std(accuracy_time_all_iter, axis=0)\n",
    "\n",
    "        print(this_area + ', ' + this_cre + ', %d cells, Mean Test accuracy: %0.4f (+/- %0.4f)' \n",
    "              %(num_cells_per_cre_area[aa,c], mean_accuracy_per_cre_area[aa,c],std_accuracy_per_cre_area[aa,c]))\n",
    "\n",
    "print('----Done----')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = num_cells_per_cre_area.astype(int)\n",
    "dataframe = pd.DataFrame(index=area_labels,columns=cre_lines,data=data)\n",
    "\n",
    "dataframe\n",
    "filename = 'number of cells per areas per cell type.h5'\n",
    "with h5py.File(filename,'w') as hf:\n",
    "    hf.create_dataset('num_cells', data=dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3, figsize=(20, 6))\n",
    "axs = axs.ravel()\n",
    "chance = (100/float(118))*np.ones(len(xtime))\n",
    "colors = ['red','blue','green','cyan']\n",
    "\n",
    "for cc in range(num_cre_lines):\n",
    "    for aa in range(num_areas):\n",
    "        axs[cc].plot(xtime, mean_accuracy_time_per_cre_area[cc,:,aa], label=area_labels[aa],color=colors[aa])\n",
    "        upper = mean_accuracy_time_per_cre_area[cc,:,aa] + std_accuracy_time_per_cre_area[cc,:,aa]\n",
    "        lower = mean_accuracy_time_per_cre_area[cc,:,aa] - std_accuracy_time_per_cre_area[cc,:,aa]\n",
    "        axs[cc].fill_between(xtime,upper,lower, alpha=0.15,color=colors[aa])\n",
    "    axs[cc].plot(xtime,chance,ls='--',color='black',label='chance')\n",
    "\n",
    "    axs[cc].spines[\"top\"].set_visible(False)\n",
    "    axs[cc].spines[\"right\"].set_visible(False)\n",
    "    axs[cc].get_xaxis().tick_bottom()\n",
    "    axs[cc].get_yaxis().tick_left()\n",
    "    axs[cc].tick_params(axis='both',direction='out')\n",
    "    axs[cc].set_title(cre_lines[cc],fontsize=18)\n",
    "    axs[cc].set_ylim(0,100)\n",
    "\n",
    "    axs[cc].set_xlabel('Time from stimulus onset (ms)', fontsize=20)\n",
    "    axs[cc].set_ylabel('Accuracy (%)', fontsize=20)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc=2, borderaxespad=0.,frameon=False,fontsize=16)\n",
    "\n",
    "with PdfPages('Decoder accuracy per visual area per cell type1'+ add_filename +'.pdf') as pdf:\n",
    "    pdf.savefig(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3, figsize=(20, 6))\n",
    "axs = axs.ravel()\n",
    "chance = (100/float(118))*np.ones(len(xtime))\n",
    "colors = ['red','blue','green','cyan']\n",
    "\n",
    "for cc in range(num_cre_lines):\n",
    "    for aa in range(num_areas):\n",
    "        axs[cc].plot(xtime, mean_accuracy_time_per_cre_area[cc,:,aa], label=area_labels[aa],color=colors[aa])\n",
    "        upper = mean_accuracy_time_per_cre_area[cc,:,aa] + std_accuracy_time_per_cre_area[cc,:,aa]\n",
    "        lower = mean_accuracy_time_per_cre_area[cc,:,aa] - std_accuracy_time_per_cre_area[cc,:,aa]\n",
    "        axs[cc].fill_between(xtime,upper,lower, alpha=0.15,color=colors[aa])\n",
    "    \n",
    "    axs[cc].plot(xtime,chance,ls='--',color='black',label='chance')\n",
    "   \n",
    "    axs[cc].spines[\"top\"].set_visible(False)\n",
    "    axs[cc].spines[\"right\"].set_visible(False)\n",
    "    axs[cc].get_xaxis().tick_bottom()\n",
    "    axs[cc].get_yaxis().tick_left()\n",
    "    axs[cc].tick_params(axis='both',direction='out')\n",
    "    axs[cc].set_title(cre_lines[cc],fontsize=20)\n",
    "\n",
    "    axs[cc].set_xlabel('Time from stimulus onset (ms)', fontsize=20)\n",
    "    axs[cc].set_ylabel('Accuracy (%)', fontsize=20)\n",
    "    \n",
    "plt.legend(bbox_to_anchor=(1, 1), loc=2, borderaxespad=0.,frameon=False,fontsize=16)\n",
    "\n",
    "with PdfPages('Decoder accuracy per visual area per cell type2'+add_filename+ '.pdf') as pdf:\n",
    "    pdf.savefig(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(11,7))\n",
    "ax = plt.subplot(111)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.get_xaxis().tick_bottom()\n",
    "ax.get_yaxis().tick_left()\n",
    "ax.tick_params(axis='both',direction='out')\n",
    "#ax.set_ylim(0,50)\n",
    "\n",
    "index = np.arange(num_areas)\n",
    "bar_width = 0.25\n",
    "opacity = 0.35\n",
    "colors =['#F78F1E','#FFC222','#EE3224']\n",
    "for cc in range(num_cre_lines):\n",
    "    plt.bar(index+(cc*bar_width),mean_accuracy_per_cre_area[:,cc],width=bar_width,alpha=opacity, color=colors[cc],\n",
    "            label=cre_lines[cc],yerr=std_accuracy_per_cre_area[:,cc])\n",
    "\n",
    "ax.set_ylim(0,100)\n",
    "chance = (100 * 1 /(1 + num_classes)) * np.ones(len(index))\n",
    "xs = np.linspace(0,4,num=4)\n",
    "plt.plot(xs, chance, ls='--', color='black',label='chance')\n",
    "plt.ylabel('Accuracy (%)',fontsize=20)\n",
    "plt.xticks(index + 1.5 * bar_width,area_labels)\n",
    "plt.grid()\n",
    "plt.legend(bbox_to_anchor=(0.95, 1), loc=2, borderaxespad=0.,frameon=False,fontsize=16)\n",
    "\n",
    "with PdfPages('Decoder accuracy per visual area per cell type3'+add_filename+'.pdf') as pdf:\n",
    "    pdf.savefig(fig) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
